# Storing Big Blobs Without Big Problems  BLOBs üòµ

After getting the core key-value store and the event system running on top of Raft, I hit the next logical wall: where do I put the big stuff? I'm talking about files, images, binaries‚Äîthe kind of data that has no business being crammed directly into a Raft log. The key-value store is great for small, atomic pieces of metadata, but trying to shove a multi-megabyte file through the consensus protocol is a fantastic way to bring the entire cluster to its knees.

The obvious answer is "just use S3, you moron." And yeah, for a production system at scale, that's probably the right call. But "how hard could it be?"
I already had a perfectly good distributed state machine and an event bus. Why couldn't I build my own distributed blob storage on top of it? It felt like the next logical step in my quest to avoid using off-the-shelf tools and instead roll my own just for the sheer hell of it.

## The Upload Shuffle üï∫

The first problem to solve is ingestion. How does a user upload a file, and how do we store it without creating chaos? I decided to stick with the pattern that was already working well for the key-value store: any node can accept a request, but only the leader can write data.

1.  A client sends a file via a standard `multipart/form-data` POST request to any node in the cluster.
2.  If that node isn't the Raft leader, it doesn't even look at the data. It just shoots back an HTTP redirect to the client, pointing it to the current leader. My client library handles this automatically, so the user never notices.
3.  The leader gets the redirected request and the real work begins. First, it checks if the user's API key has enough disk quota. No free rides.
4.  Then, it streams the uploaded file to a temporary location on its local disk. While it's streaming, it does something crucial: it calculates the `SHA256` hash of the file's content on the fly. This gives me a checksum that I can use to verify data integrity later, without having to trust the client or read the file a second time.
5.  Once the file is fully received, it's atomically moved from its temporary path to its final resting place: `<insid_home>/<node_id>/blobs/<data_scope_uuid>/<sha256_of_key>`. Hashing the key is a neat little trick to avoid filesystem weirdness with funky characters in keys.
6.  With the physical file safe on the leader's disk, the leader creates a metadata record for it‚Äîa simple struct containing the key, size, that `SHA256` hash we just calculated, and the owner's ID. This metadata‚Äîand *only* the metadata‚Äîis committed to the distributed key-value store via a Raft `Set` command.
7.  Finally, the leader uses the event bus it already has. It publishes an event with the topic `_:blob:uploaded`, and the payload is the blob's metadata.

This process neatly separates the heavy lifting (file I/O) from the consensus-critical part (metadata). The Raft log only ever sees a tiny metadata record and an event notification, keeping it fast and lean.

## Spreading the Gospel (Replication) üó£Ô∏è

So the leader has the file. Great. But this is a *distributed* system. How do the followers get their copies?

This is where that `_:blob:uploaded` event becomes so damn useful. The `blobService` running on every node in the cluster is subscribed to that topic.

When a follower node sees that event, it triggers a simple chain of logic:
1.  It checks the event payload for the blob's metadata.
2.  It asks itself: "Do I already have this file?"
3.  If the answer is no, it uses the source node ID from the metadata to open a direct, internal-only, super-secret-handshake HTTP connection to the node that has the file. It calls a special endpoint (`/db/internal/v1/blob/download`) to pull the binary data across.
4.  After downloading, it verifies the file's content by calculating its `SHA256` hash and comparing it to the hash from the event metadata. If they match, the follower saves the file to its own local storage. If they don't, something is horribly wrong, and the file is discarded.

This event-driven approach means replication happens automatically and asynchronously. The leader doesn't have to manage pushing the file to every follower; it just shouts "I got a new file!" into the void, and the followers take care of themselves.

And what about updates? If you upload a file with an existing key, the process is identical. The leader overwrites its local file and publishes a new `_:blob:uploaded` event with the new metadata (and new hash). When followers get this event for a file they already have, they just re-run the hash check. If their local hash doesn't match the new hash in the event, they know their copy is stale and pull down the new version. Simple.

## Taking Out the Trash (Deletion) üóëÔ∏è

Deleting a file in a distributed system is scarier than it sounds. If you have a massive file and you just try to delete it, what happens if the delete operation on the Raft log gets applied, but the node crashes before it can remove the physical file? You get orphaned data. Worse, what if the deletion process itself takes a long time? You could stall the Raft FSM, which is a cardinal sin.

The solution, once again, was to borrow from a pattern I'd already built: tombstoning.

When a client sends a `DELETE` request for a blob, it gets routed to the leader, but the leader **does not** delete anything. Instead, it does two things:
1.  It creates a **tombstone record** in the key-value store. This is just a tiny key like `_:blob:tombstone:<blob_key>`. This write is replicated via Raft, and as soon as it's committed, any future `GET` request for that blob will see the tombstone and return a `404 Not Found`. The blob is effectively dead to the outside world.
2.  It then relies on the `blobService`'s periodic background janitor task. On the leader node, and *only* on the leader, this janitor scans for these tombstone records.

When the janitor finds a tombstone, it orchestrates the final, graceful cleanup:
1.  It publishes a `_:blob:deleted` event to the Raft log, containing the dead blob's metadata.
2.  It commits a `Delete` command to Raft to remove the original blob metadata.
3.  It commits a final `Delete` command to Raft to remove the tombstone itself.

Every node in the cluster is listening for that `_:blob:deleted` event. When they receive it, they use the metadata to find the physical file on their local disk and simply `os.Remove` it.

This two-phase deletion process turns a dangerous, potentially blocking operation into a safe, asynchronous background task. It ensures that data is removed consistently and that the system remains responsive, no matter how large the files are. It's another case of solving a distributed systems problem by not solving it immediately, but instead, leaving detailed
breadcrumbs and utilizing different workers like janitors to build a robust system.