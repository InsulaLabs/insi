As a developer all of my professional experience is in C++. I've been doing it since 2018 "professionally" meaning that my title has represented the work since then, though my years as an IT Admin had me doing C++ and Python for my time attending University.
Recently I've found myself in a position of "self employment" where I've been doing consulting for some folks and decided to try my hand at web stuff again.
Its been a minute since I've worked on a backend that anyone reading this is likely familiar with. My "backend" (non embedded) experience has been working on a "real-time" time-series database and then working in "real-time" chat. I quote "real-time" because compared to the world of micro controllers there's nothing "real-time" about any of it.. as its arguable if you can ever actually have "real time" but I digress. Bottom line is I had no idea what was going on like 2 months ago. Last time I looked at web I think I used PHP 5 on a LAMP stack. I had no idea what react was or why people were seriously expecting folks to pull a docker instance to start a redis, some db nonsense, and a million tools just to host a couple users.
While reading why people thought they needed redis and mongodb or whatever I came across hashicorp's `raft` consensus algorithm in go. Something tickled me and away I went. I just.. started building. I wanted to learn a bit about how raft worked but more importantly I just wanted to roll my own utility and roll I did. I "should" have just used redis, but wheres the fun in that? What would I have learned? How to use a tool? No. Instead of using a few tools I'll just write a few thousand lines of code.

    After all, how hard could it be?

Getting Started üõ†Ô∏è
The first stage was getting SOMETHING running and figuring out how the raft finite state machine (fsm) logic worked, and thankfully it was straight forward. Essentially, the consensus part of Raft is just a replicated log. The library handles all the distributed systems leader election (who gets to be the one who actually writes), log replication (telling the followers to do the same operations), commitment‚Äîso you don't have to. Your only job is to provide an FSM that knows how to take a command from that log and apply it to your own application's state.

Data Storage üíæ
When a client wants to do something, say `SET my-key "some-value"`, I'd serialize that into a simple command structure, something like `{"op":"set","key":"my-key","value":"some-value"}`, and that becomes a log entry.
But what happens if the client sends that `SET` command to a follower node instead of the leader? Raft is strict: only the leader can accept writes. Instead of just failing the request, the follower node does something clever. It responds with an HTTP redirect, pointing the client to the current leader. I built the Go client to handle this automatically. It just follows the redirect and resubmits the request to the correct node without the user ever knowing. It's a simple trick that makes the cluster much easier to work with. You don't need a separate service discovery layer to find the leader; you can just point the client at any node for writes and it will find its way. The same principle provides a simple form of load balancing for reads. Since any node in the Raft cluster can serve reads from its local state, you can give the client a list of all nodes. For any read operation, it just picks one at random, spreading the read traffic across the entire cluster.
The Raft leader writes it to its log, replicates it to its followers, and once a majority of nodes have it, the entry is "committed." At that point, the Raft library calls the `Apply` method on your FSM with that log entry on every node in the cluster. My `Apply` method would just parse the command and write the data to a local BadgerDB instance. Because Raft guarantees every node applies the same logs in the same order, you get a fault-tolerant, replicated state machine almost for free. It was trivial to add more operations, so I quickly extended the command set to cover not just `set`, `get`, and `delete`, but also atomic operations like `setnx` (set if not exists) and `cas` (compare-and-swap), exposing the power of the underlying database directly through the consensus layer.
It was quick work to then make a separate badger instance - one that was in-memory only. Having the same interface means the payload over raft is the same, its just that the destination (memory) is different (disk.)

Snapshots üì∏
Once groking how data was to be dispersed and maintained I learned about the "snapshot" system.
The Raft log is great, but it can't just grow forever. If it did, a new server joining the cluster would have to replay every single operation since the dawn of time, which would be very slow. And if a server restarted, it would have a giant log to read. This is where snapshots come in. A snapshot is just a point-in-time dump of the entire state of the application. It's like a save game file. Instead of replaying the entire game from the start, a new player can just load the save and be right where everyone else is.
The Raft library periodically tells my FSM, "Hey, give me a snapshot." My job was to implement a function that could iterate over everything in both the on-disk and in-memory databases and just... dump them out into a single stream. Raft takes that stream, saves it, and then safely truncates its own log. When a new node joins, the leader doesn't send it a million log entries; it just sends the latest snapshot file. The new node receives it, I have another function that reads the stream, and it populates its own databases from the dump. It was another one of those "that's it?" moments. Hashicorp's library handles the "when" and "how" of transferring the snapshot; I just had to provide the "what".
Events ‚ÄºÔ∏è
With the key-value store and the in-memory cache humming along on top of Raft, I started thinking about real-time notifications. I needed a pub/sub system. My first thought was to pull in something like NATS or another message queue, but then I had a moment of clarity, or maybe just laziness. I already had a distributed log that reliably sent commands to every node in the cluster. Why the hell wouldn't I just use that?
So that's what I did. I added a new command type: `PUBLISH`. When the leader gets a request to publish an event, it just writes a `PUBLISH` entry to the Raft log. Every node, upon applying that log entry, doesn't write to a database. Instead, it just takes the event payload and blasts it out to any poor soul connected to it via a WebSocket. Instant distributed event bus, zero new dependencies. It was disgustingly simple and the number of permitted socket connections/ active subscribers is configurable. 
But I couldn't just let events pile up in the Raft log forever. That would defeat the purpose of snapshots and make the log a bloated mess. The solution was as devious as it was simple. When my FSM's `Apply` function sees an event command, it first checks a timestamp I slapped on the payload. If the event is more than 30 seconds old, it gets yeeted into the void. This handles the case of a recovering node replaying a backlog of logs‚Äîwe don't want it spewing ancient history to clients. And when it comes time to create a snapshot? I just don't include the events. They're not part of the persistent state. They live fast, die young, and never make it into the family photo album. It's the perfect system for ephemeral, fire-and-forget messaging.
Rolling Development & Testing My Sanity üß™
My process was what you might call "functionality-driven." I sure as hell didn't do test-driven development. I'd build a whole feature, get it to a state where it "seemed" to work, and then I'd try my damnedest to break it. This wasn't about elegant unit tests; this was about functional, end-to-end integration tests that mirrored how the thing would actually be used. It's all just word salad for "does this feature work or not?"
The first line of defense was a gauntlet of crusty bash scripts. When I finished the core API for the value store, I didn't mock a goddamn thing. I wrote scripts that would spin up a full, multi-node cluster from scratch, use the command-line tool to hammer the `set`, `get`, and `delete` endpoints, check the output, and then burn the whole thing to the ground. It was tedious, but it was real. This approach meant I was testing everything at once: the server, the Raft consensus, the client binary, the whole shebang. If there was a bug hiding in any corner of the stack, these tests would sniff it out. I repeated this process for every major feature‚Äîthe cache, the atomic operations, the event system.
Of course, bash is a nightmare for anything complex. Once I decided to bolt on a JavaScript VM to give myself a more civilized scripting environment, I inherited a new problem: I had to test the testing tool. This gave birth to a second, parallel test suite written entirely in JavaScript. These tests run inside the VM and do much of the same work as the bash scripts‚Äîhitting the API, manipulating data‚Äîbut they do it through the JS client library.
Some might call having two full integration test suites redundant. I call it necessary. It's a two-pronged attack. The bash tests prove that the core server and its raw binary interface are solid. The JS tests prove that the fancier scripting abstraction I built on top isn't a house of cards. One suite keeps the foundation from cracking, the other makes sure the facade isn't bullshit. In a distributed system, you need that kind of defense in depth to hold back the tide of manure you inevitably create.
And once I had the core stable, I added some creature comforts. To get a new user started, I didn't want them to have to manually craft a config file or muck around with `openssl` to generate certificates for TLS. So I added a `--new-cfg` flag to the server binary that spits out a sensible `cluster.yaml`, and if it detects its key and certificate files are missing on first boot, it just generates them. Little things like that saved my own sanity down the line.
Data Segmentation and Usage Restrictions üîû
Early on, I realized that if this thing was going to be useful for more than just my own tinkering, it needed a way to not let one user's bullshit stomp all over everyone else's. A free-for-all database is a recipe for disaster. This led me down the path of what I call "Entities".
Think of an Entity as a hermetically sealed container for a user's data. When you want to onboard a new app or user, you don't give them a database password; you generate an Entity for them. This spits out a unique API key, and that key is their entire world. Every single API call they make‚Äîevery `set`, `get`, `publish`, whatever‚Äîis automatically scoped to their Entity. There is no way for Entity A to even know that Entity B exists, let alone read or write its data. A sort of multi-tenancy. Each entity key breaks down into two UUIDs. One denoting the existence of the key itself, and another being the "data scope" to the key. A "data scope" is a fancy way to say "we prefix all your keys with some arbitrary unique value that you don't get to ever see."
And who gets to create these Entities? An administrative "root" user. The config has a simple `instanceSecret` field. A hash of that secret becomes the `rootApiKey`, a master key with the power to manage the cluster. Of course, just walling off the data isn't enough. You also have to keep people from using all the goddamn resources (especially memory as we expose the cache!) So, I built a two-tiered system of restrictions. First, there's the blunt instrument: global rate limiting. Right in the config, you can set hard limits on how many requests per second the entire cluster will handle for each service‚Äîpersistent storage, cache, events, etc.
The second layer is more granular: per-Entity quotas. When the root user creates an Entity, they can slap a whole set of hard limits on it. How many bytes can they actually store on disk (`BytesOnDisk`)? How much space can they take up in the in-memory cache (`BytesInMemory`)? How many events can they spew out (`EventsEmitted`) before we cut them off for the day? It's all tracked in real-time. To do this efficiently, I went back to my "roll your own" philosophy. Instead of doing a read-modify-write dance to update a usage counter, which would be slow and prone to race conditions, I added another custom command to the Raft FSM: `bump`. It's a fire-and-forget operation to atomically increment or decrement a value. If a user tries to write one byte over their `BytesOnDisk` limit, the `bump` operation that checks the limit fails, and the request is rejected before it even touches the main value store. Simple as that. It forces a certain discipline and prevents any single user from hogging all the disk or memory and screwing over the entire cluster. Given I'd already built the atomic primitives, it was easy üëèüëè peazy üëèüëè
Data Cleanup and "Tombstoning" ü™¶
So what happens when you want to delete an entity? In a normal, sane application, you'd just issue a `DELETE` command and be done with it. But in a distributed system, that's a recipe for a bad time. What if that user has millions of keys and gigabytes of data? Running a delete operation that massive would lock up the single-threaded Raft FSM, effectively stalling the entire cluster while it churns. Everything would grind to a halt.
The solution is to not actually delete anything right away. Instead, we use a process called "tombstoning." When an admin issues a delete command for an API key (entity), all the system does is write a new key to the database, something like `tombstone:<api-key-id>`. That's it. It's a tiny, fast write to the Raft log that basically says, "this key is dead to me." The key immediately stops working for any new requests, but all its data is still sitting there, taking up space.
This is where the janitor comes in. I wrote a background process called the `tombstoneRunner` that only ever runs on the cluster leader. Every 30 seconds or so, it wakes up and scans for these tombstones. When it finds one, it doesn't try to delete everything at once. Instead, it starts methodically cleaning up the dead entity's data in small batches. It'll delete a hundred keys, then go back to sleep. On the next cycle, it'll delete another hundred. This asynchronous, piecemeal approach ensures that the cleanup work never gets in the way of live traffic.
Once the runner has finished wiping all of an entity's primary data, it moves on to the metadata‚Äîthe usage trackers, the quota limits, all that administrative cruft. After that's gone, it deletes the API key record itself. And the very last thing it does is delete the tombstone marker, scrubbing the final trace of the entity from the system. It turns a potentially catastrophic, blocking operation into a safe, non-disruptive background task. It's the only sane way to handle deletions at scale without shooting yourself in the foot.
What's Next? üöß
So, what started as a "how hard could it be?" descent into Go and Raft has turned into... this. A custom-built, distributed data store that's probably way less feature complete than what is freely available, but it's *mine*. I built it from the ground up, not because I thought I could do better than Redis, but because I wanted to tackle the problem in go and explore the space around raft. Instead of learning an API, I learned about the raft consensus, their state machines, and got to roll in the mud of janitorial work required to keep a distributed system from eating itself.
The question now is what to do with it. My plan is to release Insi as an open-source project. Full disclosure: while I've been developing software for a while, I'm a complete novice when it comes to actually running something open-source; my contributions have always been one-off commits to other people's stuff or "absolutely proprietary," so please be patient while I figure out what's up. But that won't happen until I've put it through its paces. I intend to spend the next few months battle-testing it in a real production environment, letting it get punched in the face by actual users so I can iron out the inevitable kinks and stupid mistakes I've made. The goal is to have something reasonably stable to share towards the end of the summer.
That being said, if you've read this far and you're the kind of person who enjoys tinkering with tools that still have that "new car" smell mixed with a faint whiff of "imminent immolation," I'm open to releasing it sooner. If there's genuine interest, I can push the code to a public repository with the massive caveat that it's not yet considered stable.
For now, it remains my bespoke solution to a problem I could have solved with two lines of `docker-compose`.
But where's the fun in that?