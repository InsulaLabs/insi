# InsiDB: System Overview

## 1. Introduction

InsiDB is a distributed key-value (K/V) data store, offering functionalities reminiscent of systems like Redis, but with its own unique features tailored for specific use cases. It is designed to be interfaced with over HTTPS and utilizes the Raft consensus algorithm to synchronize operations across a cluster of nodes, ensuring data consistency and fault tolerance.

Core features include:
*   **Distributed K/V Storage**: Data can be replicated across multiple nodes.
*   **Raft Consensus**: Guarantees strong consistency for data operations.
*   **Tagging**: Allows associating multiple tags with keys for flexible querying (`tag -> key` lookups).
*   **TTL Cache**: An in-memory cache layer for ephemeral key-value data with Time-To-Live (TTL) semantics, consistently managed across the cluster.
*   **HTTP API**: A simple, RESTful API for all data and system operations.
*   **Security**: Operations are secured via API keys, with a root-level administrative token derived from a shared instance secret. Communication can be secured using TLS.

The primary purpose of InsiDB is to provide a reliable, consistent, and relatively simple distributed data storage and caching solution for applications.

## 2. Core Concepts

### 2.1. Distributed Key-Value Store
InsiDB allows data (keys and their associated string values) to be stored and replicated across multiple server nodes. This distribution provides high availability and fault tolerance; the system can continue to operate even if some nodes fail (up to a limit tolerated by Raft).

### 2.2. Raft Consensus
At the heart of InsiDB's consistency model is the Raft consensus algorithm.
*   **Leader Election**: Nodes elect a leader responsible for coordinating client requests.
*   **Log Replication**: All write operations (setting a key, tagging, cache operations) are first committed to a replicated log on the leader, then sent to follower nodes. An operation is only considered committed once a majority of nodes have acknowledged it.
*   **State Machine Application**: Once a log entry is committed, it's applied to a finite state machine (FSM) on each node. This ensures that all nodes apply the same operations in the same order, leading to identical data states.
*   **Fault Tolerance**: Raft allows the cluster to continue functioning as long as a majority of nodes are operational.

### 2.3. Tagging
Beyond simple key-value storage, InsiDB supports associating multiple string "tags" with any given key.
*   A key can have many tags.
*   A tag can be associated with many keys.
*   This creates a `tag -> [keys]` index, allowing clients to retrieve all keys associated with a specific tag.
*   This is useful for categorizing data or creating secondary indices.

### 2.4. TTL Cache Mechanism
InsiDB includes an in-memory Time-To-Live (TTL) cache for ephemeral data.
*   **Purpose**: To reduce latency for frequently accessed data and to offload requests from the persistent disk-based storage.
*   **`SetAt` for Consistency**: When a cache item is set, the operation is replicated via Raft. A crucial field, `SetAt` (a timestamp), is set by the leader node *just before* the cache operation is proposed to the Raft log. When follower nodes apply this replicated cache operation, they use this original `SetAt` timestamp and the provided TTL (in seconds) to calculate the item's absolute expiry time. This ensures that all nodes in the cluster will expire the cache item simultaneously, maintaining cache consistency.
*   **No Bump-on-Hit**: The TTL of a cache item is not extended ("bumped") when it is read. This simplifies the cache synchronization logic, as reads are local and do not require Raft consensus or propagation. Once an item is set with a TTL, its expiry time is fixed across the cluster.
*   **Replication**: Cache set and delete operations are Raft-replicated commands, ensuring all nodes maintain a consistent view of the cache's intended state (though items may be evicted locally due to memory pressure, the Raft log dictates the "source of truth" for cache entries and their expiries).

### 2.5. API Keys
Client access to InsiDB is controlled by API keys.
*   **Generation**: API keys are generated by an administrator (using the admin token) for a specific, arbitrary "entity" string (e.g., an application name or user ID).
*   **Structure**: Each key contains a UUID and metadata, cryptographically tied to the cluster's `instanceSecret` via the `sentinel` library.
*   **Authentication**: Clients present their API key in the `Authorization` HTTP header. The service validates the key against its internal records and its cryptographic structure.
*   **Authorization**: While the current implementation primarily uses API keys for authentication, the "entity" information embedded in the key could be used for finer-grained authorization in the future. Currently, a valid API key grants access to the general K/V, tagging, and caching operations.
*   **Local Caching**: Validated API keys (and their associated entity/UUID) are cached in memory by each service node for a short duration (`cache.keys` TTL in `cluster.yaml`) to reduce the overhead of repeated validation against the persistent store.

### 2.6. Instance Secret & Root Prefix
*   **`instanceSecret`**: A high-entropy string shared by all nodes in the `cluster.yaml` configuration.
    *   Its SHA256 hash serves as the **administrator token** (also referred to as `authToken` internally). This token is used for privileged system-level operations, such as:
        *   Creating and deleting API keys.
        *   Authorizing node join requests during the auto-join process.
        *   Used by the `insic` CLI tool by default for its operations.
    *   It's also a critical component in the cryptographic construction and deconstruction of API keys via the `sentinel` library, ensuring that API keys are specific to this InsiDB instance.
*   **`rootPrefix`**: A configuration value from `cluster.yaml`.
    *   It's used as an internal namespace for data related to the system itself, particularly for storing and managing API key metadata within the underlying database. When a new API key is created, its reference is stored under keys prefixed with this `rootPrefix`.
    *   It's also returned as the "entity" when the admin token is validated.

## 3. Architecture

InsiDB comprises server nodes (`insid`), a client CLI (`insic`), and a Go client library.

### 3.1. Nodes (`insid`)
Each InsiDB server node runs an `insid` process.
*   **Identity**: Each node is identified by a unique Node ID (e.g., `node0`, `node1`).
*   **Configuration**: Node-specific settings like network bindings (`raftBinding`, `httpBinding`) and a `nodeSecret` are defined in `cluster.yaml`.
*   **`nodeSecret`**: Used to encrypt a node-specific identity file (`<nodeId>.identity.encrypted`), which contains cryptographic keys for the node (its "badge").
*   **Data Storage**: Each node maintains its own data directory (configured via `insudbDir` in `cluster.yaml`, typically `insudbDir/<nodeId>/`). This directory houses:
    *   Raft logs (for consensus).
    *   BadgerDB databases for persistent key-value data and tag-to-key mappings (managed by the `tkv` library).
    *   The encrypted node identity file.

### 3.2. Configuration (`cluster.yaml`)
This YAML file is central to the cluster's setup. Key fields include:
*   `instanceSecret`: Shared secret for admin token generation and API key cryptography.
*   `rootPrefix`: Namespace for internal system data (e.g., API key metadata).
*   `defaultLeader`: Node ID of the default leader, used by other nodes for auto-joining on first launch.
*   `insudbDir`: Root directory for all persistent data storage.
*   `tls`:
    *   `cert`: Path to TLS certificate file.
    *   `key`: Path to TLS private key file.
*   `clientSkipVerify`: If `true`, clients (including nodes attempting to join) will skip TLS certificate verification.
*   `serverMustUseTLS`: If `true` and TLS cert/key are provided, the server will only serve HTTPS.
*   `cache`:
    *   `standard-ttl`: Default TTL for the `tkv` library's main cache (used for general caching if not overridden).
    *   `keys`: TTL for the in-memory cache of validated API keys within the `Service` component.
*   `nodes`: A map where each key is a Node ID (e.g., `node0`) and the value contains:
    *   `raftBinding`: `host:port` for Raft communication.
    *   `httpBinding`: `host:port` for the client-facing HTTP API.
    *   `nodeSecret`: Unique secret for this node to encrypt its identity.

The configuration is loaded and validated at startup by `config.LoadConfig()`.

### 3.3. Server (`insid`)

The `insid` executable runs the InsiDB server node.

#### 3.3.1. Startup Process (`cmd/insid/main.go`)
1.  **Parse Flags**: Reads command-line arguments, primarily `-config` (path to `cluster.yaml`) and `-as` (the Node ID this instance should run as).
2.  **Load Configuration**: Loads `cluster.yaml` using `config.LoadConfig()`.
3.  **Node Identification**: Retrieves its specific configuration (`nodeSpecificCfg`) from the loaded cluster config using the `-as` Node ID.
4.  **Directories**: Creates `insudbDir` (if it doesn't exist) and the node-specific data subdirectory (`insudbDir/<nodeId>/`).
5.  **Node Identity (Badge)**: Calls `loadOrCreateBadge()`. This attempts to load an encrypted identity file (`<nodeId>.identity.encrypted`) from the node's data directory using its `nodeSecret`. If not found, it generates a new cryptographic identity (badge) and saves it encrypted.
6.  **Initialize `tkv`**: Instantiates the `tkv` manager (`tkv.New()`), providing it with the node's identity, logger, data directory for BadgerDB files, application context, and the `clusterCfg.Cache.StandardTTL`. The `tkv` library manages the underlying BadgerDB instances (one for values, one for tags) and the main TTL cache.
7.  **Initialize Service (`service.NewService()` in `internal/service/service.go`)**:
    *   Creates the main `Service` struct.
    *   **Raft Initialization (`rft.New()` in `internal/rft/fsm.go`)**: This is a critical step.
        *   Sets up the Raft instance for the node, configuring its log store, stable store (for Raft metadata), and transport layer for communication with other Raft nodes.
        *   Initializes the Key-Value Finite State Machine (`kvFsm`), which implements `raft.FSM` and interacts with the `tkv` store.
        *   **Auto-Join**: If it's the node's first launch (determined by the absence of a lock file `insudbDir/<nodeId>.lock`) and the node is not the `defaultLeader`, it attempts to automatically join the cluster by contacting the `defaultLeader`'s HTTP API (`/db/api/v1/join`). This join request is authenticated using the admin token (derived from `instanceSecret`). The leader, upon receiving a valid join request, adds the new node as a voter to the Raft configuration.
        *   Creates the lock file after successful initialization.
    *   Initializes local caches within the `Service` struct, specifically `lcs.apiKeys` (a `ttlcache.Cache` for validated API keys).
    *   Calculates the `authToken` (admin token) from the `clusterCfg.InstanceSecret`.
8.  **Run Service (`srvc.Run()`)**:
    *   Registers all HTTP handlers for the API endpoints.
    *   Starts the HTTP/HTTPS server, listening on the configured `httpBinding`.
    *   Handles graceful shutdown on context cancellation (e.g., OS interrupt signal).

#### 3.3.2. HTTP API (`internal/service/service.go`)
The service exposes a RESTful HTTP API, typically under the `/db/api/v1/` prefix.
*   **Authentication**: All handlers (except perhaps a public ping, though current ping is authed) first call `s.validateToken(r, mustBeRoot)` to authenticate the request. This checks the `Authorization` header for either the admin token or a valid API key.
*   **Value Handlers**:
    *   `POST /db/api/v1/set`: Sets a key-value pair.
    *   `GET  /db/api/v1/get`: Retrieves a value by key.
    *   `POST /db/api/v1/delete`: Deletes a key.
    *   `GET  /db/api/v1/iterate/prefix`: Iterates keys by a given prefix.
*   **Tagging Handlers**:
    *   `POST /db/api/v1/tag`: Associates a tag with a key.
    *   `POST /db/api/v1/untag`: Removes a tag association from a key.
    *   `GET  /db/api/v1/iterate/tags`: Iterates keys by a given tag.
*   **Cache Handlers**:
    *   `POST /db/api/v1/cache/set`: Sets a key-value pair in the TTL cache.
    *   `GET  /db/api/v1/cache/get`: Retrieves a value from the cache by key.
    *   `POST /db/api/v1/cache/delete`: Deletes a key from the cache.
*   **System Handlers**:
    *   `GET  /db/api/v1/join`: Allows a new follower node to join the Raft cluster (called by the follower, handled by the leader). Requires admin token.
    *   `GET  /db/api/v1/new-api-key`: Generates a new API key for a given entity. Requires admin token.
    *   `GET  /db/api/v1/delete-api-key`: Deletes an API key. Requires admin token.
    *   `GET  /db/api/v1/ping`: An authenticated ping to check server and token validity.

If a write operation is received by a follower node, it typically forwards the request to the current Raft leader or instructs the client to do so. The `deleteApiKey` handler explicitly demonstrates client-side forwarding logic if the current node is not the leader. Read operations can often be served by any node, though this depends on the desired consistency (strong vs. stale reads).

#### 3.3.3. Raft & FSM (`internal/rft/fsm.go`)
The `kvFsm` struct is the implementation of `raft.FSM`.
*   **Raft Commands**: Write operations are encapsulated into `RaftCommand` structs, which have a `Type` (e.g., `cmdSetValue`, `cmdSetCache`) and a `Payload` (the marshaled data model, like `models.KVPayload` or `models.CachePayload`).
*   **`Apply(log *raft.Log)`**: This is the core of the FSM. When Raft commits a log entry, this method is called on all nodes.
    1.  It deserializes the `RaftCommand` from `log.Data`.
    2.  Based on `cmd.Type`, it unmarshals `cmd.Payload` into the appropriate `models` struct.
    3.  It then performs the actual data modification by calling methods on its `tkv` instance (e.g., `kf.tkv.Set()`, `kf.tkv.Tag()`, `kf.tkv.CacheSet()`).
    4.  For `cmdSetCache`, it critically uses the `SetAt` timestamp from the replicated payload and the original TTL to calculate the remaining duration, ensuring consistent expiry across all nodes.
*   **Snapshotting (`Snapshot()`, `Restore()`)**:
    *   Periodically, Raft may request a snapshot of the FSM's state to truncate its log.
    *   `Snapshot()`: Creates a `badgerFSMSnapshot`. This iterates through the `valuesDb`, `tagsDb` (from `tkv.GetDataDB()`, `tkv.GetTagDB()`), and the `stdCache` (`tkv.GetCache()`), writing their entries to a `raft.FSMSnapshotSink`. Cache entries include their original TTL and encoding time for correct restoration.
    *   `Restore()`: Clears the current state and repopulates the databases and cache from a snapshot stream. For cache items, it calculates the remaining TTL based on the stored encoding time and original TTL.
*   **Leader Operations**: The `kvFsm` also provides methods that are typically leader-initiated but are submitted to Raft for replication:
    *   `Set(kvp models.KVPayload)`
    *   `Delete(key string)`
    *   `Tag(kvp models.KVPayload)`
    *   `Untag(kvp models.KVPayload)` (payload contains key and tag to remove)
    *   `SetCache(payload models.CachePayload)` (crucially sets `payload.SetAt = time.Now()` before marshalling and proposing to Raft)
    *   `DeleteCache(key string)`
*   **Join Logic (`Join(followerId, followerAddress)`)**: Called by the leader (via the HTTP join handler) to add a new voter to the Raft cluster using `kf.r.AddVoter()`.

#### 3.3.4. Storage (`tkv` library)
The `tkv` library (external, `github.com/InsulaLabs/insi/insula/tkv`) abstracts the persistent storage and primary TTL cache.
*   **BadgerDB**: Uses BadgerDB for durable storage.
    *   `valuesDb`: Stores the primary key-value data.
    *   `tagsDb`: Stores tag-to-key mappings (e.g., `tag -> set_of_keys` or similar representation).
*   **TTL Cache (`stdCache`)**: Provides an in-memory TTL cache, likely using a library like Ristretto or a custom implementation. This is the cache manipulated by `cmdSetCache`/`cmdDeleteCache` through the FSM.

### 3.4. Client

#### 3.4.1. `insic` CLI (`cmd/insic/main.go`)
A command-line interface for interacting with an InsiDB cluster.
*   **Configuration**: Uses the same `cluster.yaml` as server nodes to find node addresses.
*   **Targeting**: The `--target <nodeId>` flag specifies which node to connect to. If not provided, it defaults to the `defaultLeader` from the config.
*   **Authentication**: By default, `getClient()` in `insic` uses the SHA256 hash of the `instanceSecret` from `cluster.yaml` as the `authToken` for its `client.NewClient()` instance. This means most CLI operations are performed with administrator privileges. The `api verify <key>` command is an exception, as it uses the provided `<key>` as the `authToken` to test its validity.
*   **Commands**: Provides a range of commands mirroring the HTTP API:
    *   `get <key>`, `set <key> <value>`, `delete <key>`
    *   `tag <key> <tag>`, `untag <key> <tag>`
    *   `iterate prefix <prefix> [offset] [limit]`, `iterate tag <tag> [offset] [limit]`
    *   `cache set <key> <value> <ttl>`, `cache get <key>`, `cache delete <key>`
    *   `join <leaderNodeID> <followerNodeID>`: Tells `leaderNodeID` to add `followerNodeID`.
    *   `api add <entity_name>`: Creates a new API key.
    *   `api delete <api_key_value>`: Deletes an API key.
    *   `api verify <api_key_value>`: Pings the server using the given key to check its validity.
    *   `ping`: Pings the target server (using admin token).

#### 3.4.2. Go Client Library (`client/client.go`)
Provides a Go API for programmatic interaction with InsiDB.
*   **`NewClient(hostPort, authToken, clientSkipVerifySetting, logger)`**: Constructor.
    *   `hostPort`: The `address:port` of the target InsiDB node's HTTP API.
    *   `authToken`: The API key or admin token to be used in the `Authorization` header.
    *   `clientSkipVerifySetting`: Boolean, if true, TLS certificate verification is skipped.
*   **Methods**: Exposes methods for all API endpoints (e.g., `Get()`, `Set()`, `Tag()`, `SetCache()`, `NewAPIKey()`, `Join()`).
*   **Request Handling**: Internally uses `doRequest` to marshal request bodies to JSON, set `Content-Type` and `Authorization` headers, make HTTP requests, and unmarshal JSON responses.
*   **TLS**: Enforces `https://` scheme for base URLs.

### 3.5. Security

#### 3.5.1. Instance Secret & Admin Token
As described in Core Concepts, the `instanceSecret` is paramount. Its hash is the admin token, granting broad privileges. Protecting the `cluster.yaml` file is crucial.

#### 3.5.2. API Keys (`internal/service/tokens.go`)
*   **Generation**: `service.newApiKey()` calls `sentinel.ConstructApiKey()`, which uses the `instanceSecret` and the provided `entity` string. The generated key is returned to the admin.
*   **Storage for Validation**: The service doesn't store the raw API key. Instead, when `newApiKey` is called, it stores a record *about* the key (using `s.assembleSystemKeyForRootStorage(key)` which incorporates `s.cfg.RootPrefix`) in the persistent `valuesDb` via an internal client `Set` and `Tag` operation. This record essentially marks the key as validly issued. The key itself can be reconstructed/validated using `sentinel`.
*   **Validation (`service.validateToken()`)**:
    1.  Checks if the `Authorization` header matches the `s.authToken` (admin).
    2.  Checks a local in-memory cache (`s.lcs.apiKeys`) for the provided API key.
    3.  If not cached, it checks the persistent store for the key's record (`s.fsm.Get(s.assemblePotentiallyStoredKey(authHeader))`). The `assemblePotentiallyStoredKey` also uses the `RootPrefix`.
    4.  If the record exists, it then uses `sentinel.DeconstructApiKey(authHeader)` with the `instanceSecret` to cryptographically validate the key and extract the original `entity` and `UUID`.
    5.  If valid, the key and its details (`TokenCache {Entity, UUID}`) are added to the `s.lcs.apiKeys` cache.

#### 3.5.3. TLS
*   **Server-Side**: `service.Run()` will start an HTTPS server if `cfg.TLS.Cert` and `cfg.TLS.Key` are provided in `cluster.yaml`. `cfg.ServerMustUseTLS` can enforce this.
*   **Client-Side (`client/client.go`)**: `NewClient()` defaults to `https://`. The `clientSkipVerifySetting` (from `clusterCfg.ClientSkipVerify`) controls whether the HTTP client skips server certificate validation.
*   **Auto-Join (`internal/rft/auto_join.go`)**: The HTTP client used for auto-join respects `clusterCfg.ClientSkipVerify`. If not skipping and a CA cert is provided in `clusterCfg.TLS.Cert`, it attempts to use that CA for verifying the leader's certificate during the join process.

#### 3.5.4. Node Secrets
Each node has a unique `nodeSecret` in `cluster.yaml`. This is used by `cmd/insid/main.go` in `loadOrCreateBadge()` to encrypt and decrypt the node's identity file (`<nodeId>.identity.encrypted`). This badge likely contains the node's own cryptographic keys used for secure communication or identification within the cluster, separate from client API keys or TLS.

### 3.6. Data Models (`models/data.go`)
These structs define the structure of data passed in API request/response bodies and FSM command payloads.
*   `KVPayload {Key string, Value string}`: For setting key-values, and for tagging (where `Value` becomes the tag).
*   `TagPayload {Key string, Tag string}`: Specifically for untag operations within the FSM context.
*   `KeyPayload {Key string}`: For operations requiring only a key, like delete.
*   `CachePayload {Key string, Value string, TTL time.Duration, SetAt time.Time}`:
    *   Used for cache set operations.
    *   `TTL`: Represents the desired lifetime of the cache item. The client (`client.SetCache`) sends this as seconds. The server's HTTP handler converts this to `time.Duration` before it's put into this struct for FSM processing.
    *   `SetAt`: Timestamp set by the Raft leader just before proposing the cache set command. Crucial for consistent expiry calculation across all nodes.

## 4. Data Flow Examples

### 4.1. Write Key-Value (`set mykey myvalue`)
1.  **Client Request**: `insic set mykey myvalue` (or Go client `c.Set("mykey", "myvalue")`) sends an HTTP POST to `/db/api/v1/set` on a target node (e.g., `node0`) with `{"key": "mykey", "value": "myvalue"}`. The `Authorization` header contains the admin token (default for `insic`).
2.  **Node Receives (e.g., `node0`)**:
    *   `setHandler` in `service.go` is invoked.
    *   `s.validateToken()` authenticates the admin token.
3.  **Leader Check & FSM Call**:
    *   If `node0` is the Raft leader: it calls `s.fsm.Set(models.KVPayload{Key: "mykey", Value: "myvalue"})`.
    *   Inside `fsm.Set()`:
        *   A `RaftCommand` is created: `{Type: "set_value", Payload: marshaled_KVPayload}`.
        *   `kf.r.Apply(marshaled_RaftCommand, timeout)` is called. This proposes the command to the Raft log.
4.  **Raft Replication**:
    *   The Raft leader (`node0`) writes the log entry locally and sends it to follower nodes (`node1`, `node2`).
    *   Followers acknowledge receipt. Once a majority acknowledge, the entry is committed.
5.  **FSM Application (All Nodes)**:
    *   Each node (leader and followers) eventually calls `kvFsm.Apply(log_entry)` for the committed entry.
    *   Inside `Apply()`:
        *   The `RaftCommand` is unmarshaled.
        *   `cmd.Type` is `cmdSetValue`. The `cmd.Payload` is unmarshaled into `models.KVPayload`.
        *   `kf.tkv.Set("mykey", "myvalue")` is called, writing to the local BadgerDB.
6.  **Client Response**: The leader node (`node0`), after `raft.Apply()` returns successfully (indicating the FSM processed it without error), responds to the client with HTTP 200 OK.
7.  **Non-Leader Node**: If the request initially hit a follower, that follower would typically redirect the client to the leader, or proxy the request.

### 4.2. Read Key-Value (`get mykey`)
1.  **Client Request**: `insic get mykey` sends HTTP GET to `/db/api/v1/get?key=mykey` to a node. `Authorization` header has admin token.
2.  **Node Receives**:
    *   `getHandler` is invoked.
    *   `s.validateToken()` authenticates.
3.  **Local FSM Read**:
    *   The handler calls `s.fsm.Get("mykey")`.
    *   `kvFsm.Get()` directly calls `kf.tkv.Get("mykey")`, which reads from the local BadgerDB. (This is a local read; for strong consistency, reads might need to go through the leader or a lease mechanism, but current code suggests direct local FSM/TKV read).
4.  **Client Response**: The node returns HTTP 200 OK with `{"data": "myvalue"}`.

### 4.3. Set Cache Item (`cache set cachekey cachevalue 60s`)
1.  **Client Request**: `insic cache set cachekey cachevalue 60s` sends POST to `/db/api/v1/cache/set` with `{"key": "cachekey", "value": "cachevalue", "ttl": 60}` (TTL as seconds). `Authorization` has admin token.
2.  **Node Receives (e.g., leader `node0`)**:
    *   `setCacheHandler` invoked.
    *   `s.validateToken()` authenticates.
    *   The handler constructs `models.CachePayload{Key: "cachekey", Value: "cachevalue", TTL: 60 * time.Second}`. (Converts client's seconds to `time.Duration`).
3.  **Leader FSM Call**:
    *   `s.fsm.SetCache(the_CachePayload)`.
    *   Inside `fsm.SetCache()`:
        *   `payload.SetAt = time.Now()` is set.
        *   A `RaftCommand` is created: `{Type: "set_cache", Payload: marshaled_CachePayload_with_SetAt}`.
        *   `kf.r.Apply(marshaled_RaftCommand, timeout)` is called.
4.  **Raft Replication**: Similar to K/V write, the command is replicated.
5.  **FSM Application (All Nodes)**:
    *   Each node calls `kvFsm.Apply(log_entry)`.
    *   Inside `Apply()` for `cmdSetCache`:
        *   `CachePayload` is unmarshaled (includes `Key`, `Value`, `TTL` as `time.Duration`, and `SetAt`).
        *   `intendedExpiryTime := p.SetAt.Add(p.TTL)`.
        *   `remainingTTLForNode := time.Until(intendedExpiryTime)`.
        *   If `remainingTTLForNode > 0`, `kf.tkv.CacheSet(p.Key, p.Value, remainingTTLForNode)` is called, adding to the local in-memory cache.
6.  **Client Response**: Leader responds with HTTP 200 OK.

## 5. Key Go Packages & Roles

*   **`cmd/insid`**: (`cmd/insid/main.go`)
    *   Entry point for the InsiDB server (`insid`).
    *   Handles command-line flags, configuration loading, signal handling, and the main application lifecycle (initializing `tkv`, `service`, and Raft).
*   **`cmd/insic`**: (`cmd/insic/main.go`)
    *   Entry point for the InsiDB command-line client (`insic`).
    *   Parses commands and arguments, initializes the `client.Client`, and executes corresponding client methods.
*   **`internal/config`**: (`internal/config/config.go`)
    *   Defines Go structs (`Cluster`, `Node`, `TLS`, `Cache`) that map to `cluster.yaml`.
    *   Provides `LoadConfig()` for reading, unmarshalling, and validating the cluster configuration file.
*   **`internal/service`**: (`internal/service/service.go`, `tokens.go`, `caches.go`)
    *   The core application layer for the server.
    *   `Service` struct: Holds application context, configuration, logger, FSM instance, TKV instance, etc.
    *   HTTP API Handlers: Implements the logic for each API endpoint.
    *   `tokens.go`: Manages API key generation, validation, and interaction with the `sentinel` library. Includes caching of validated API keys.
    *   `caches.go`: Initializes and manages local, non-Raft-replicated caches within the service layer (e.g., the API key cache).
*   **`internal/rft`**: (`internal/rft/fsm.go`, `auto_join.go`, `snapshot.go` (not provided but inferred))
    *   Raft Finite State Machine (FSM) implementation (`kvFsm`).
    *   `kvFsm.Apply()`: Applies committed Raft log entries to the `tkv` store.
    *   Handles Raft command marshalling/unmarshalling.
    *   Manages Raft snapshots (persisting and restoring FSM state).
    *   `auto_join.go`: Implements the logic for new nodes to automatically join an existing cluster by contacting the default leader.
    *   Interfaces with `github.com/hashicorp/raft`.
*   **`client`**: (`client/client.go`)
    *   The Go client library for InsiDB.
    *   Provides `Client` struct and methods to interact with all InsiDB HTTP API endpoints.
    *   Handles HTTP request creation, JSON marshalling/unmarshalling, and authentication headers.
*   **`models`**: (`models/data.go`)
    *   Defines common data structures (e.g., `KVPayload`, `CachePayload`) used in API requests/responses and as payloads for FSM commands.
*   **`github.com/InsulaLabs/insula/tkv`** (External Dependency):
    *   Library providing an abstraction over persistent key-value storage (BadgerDB) and a primary TTL-based in-memory cache. It separates concerns for "values", "tags", and the "standard cache".
*   **`github.com/InsulaLabs/insula/security/badge`** (External Dependency):
    *   Used for creating, encrypting, and decrypting node identity "badges".
*   **`github.com/InsulaLabs/insula/security/sentinel`** (External Dependency):
    *   Used for cryptographically constructing and deconstructing API keys, tying them to the `instanceSecret`.
*   **`github.com/hashicorp/raft`** (External Dependency):
    *   The core Raft consensus algorithm implementation.
*   **`github.com/jellydator/ttlcache/v3`** (External Dependency):
    *   Used by `internal/service/caches.go` for the local API key cache. The `tkv` library likely uses its own or a different caching library for its "standard cache".

## 6. Getting Started for Developers

This section provides a basic guide for new developers to set up and run a local InsiDB cluster.

### Prerequisites
*   Go (version 1.20 or later recommended).
*   Access to a terminal/shell.

### Setup
1.  **Clone Repository**:
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```
2.  **Create Configuration (`cluster.yaml`)**:
    Create a `cluster.yaml` file in the root of the project (or specify its path with `-config`). Here's an example for a 3-node local cluster:
    ```yaml
    instanceSecret: "a-very-secret-preshared-key-for-the-cluster"
    rootPrefix: "insidb-system"
    defaultLeader: "node0"
    insudbDir: "./.insudb_data" # Store data in a local directory

    tls:
      cert: "" # Optional: /path/to/cert.pem
      key:  "" # Optional: /path/to/key.pem

    clientSkipVerify: true   # For local dev, skip TLS verify if using self-signed certs
    serverMustUseTLS: false # For local dev, allow HTTP

    cache:
      standard-ttl: 60m # tkv's default cache item TTL
      keys: 2m          # API key validation cache TTL in service

    nodes:
      node0:
        raftBinding: 127.0.0.1:2222
        httpBinding: 127.0.0.1:8443
        nodeSecret: "node0-specific-secret"
      node1:
        raftBinding: 127.0.0.1:2223
        httpBinding: 127.0.0.1:8444
        nodeSecret: "node1-specific-secret"
      node2:
        raftBinding: 127.0.0.1:2224
        httpBinding: 127.0.0.1:8445
        nodeSecret: "node2-specific-secret"
    ```
    Ensure `insudbDir` is writable. Node secrets should be unique.

### Building
*   **Build Server (`insid`)**:
    ```bash
    go build ./cmd/insid/main.go
    # This creates an 'insid' executable (or 'main' if not renamed)
    ```
*   **Build Client (`insic`)**:
    ```bash
    go build ./cmd/insic/main.go
    # This creates an 'insic' executable (or 'main' if not renamed)
    ```
    You might want to rename them: `mv main insid` and `mv main insic`.

### Running Nodes
Open three separate terminals.
*   **Terminal 1 (Node 0 - Default Leader)**:
    ```bash
    ./insid -config cluster.yaml -as node0
    ```
*   **Terminal 2 (Node 1 - Follower)**:
    ```bash
    ./insid -config cluster.yaml -as node1
    ```
    Node 1 will attempt to auto-join `node0`.
*   **Terminal 3 (Node 2 - Follower)**:
    ```bash
    ./insid -config cluster.yaml -as node2
    ```
    Node 2 will attempt to auto-join `node0`.

Watch the logs for successful startup and join messages.

### Using the CLI (`insic`)
*   **Ping a node (uses admin token by default)**:
    ```bash
    ./insic -config cluster.yaml -target node0 ping
    ```
*   **Create an API key (for "my_app_entity")**:
    ```bash
    ./insic -config cluster.yaml -target node0 api add my_app_entity
    # This will output: API Key: <the_actual_api_key>
    # Copy this key.
    ```
*   **Verify the new API key**:
    ```bash
    ./insic -config cluster.yaml -target node0 api verify <the_actual_api_key_from_above>
    ```
*   **Set a value (uses admin token by default)**:
    ```bash
    ./insic -config cluster.yaml -target node0 set mykey "Hello InsiDB"
    ```
*   **Get a value**:
    ```bash
    ./insic -config cluster.yaml -target node1 get mykey
    # Output: Hello InsiDB (demonstrates replication)
    ```

To use a specific API key for general operations with `insic` (instead of the admin token), the CLI would need modification (e.g., an `--api-key` flag or environment variable support), as `getClient()` currently defaults to using the `instanceSecret`.

### Key Code Locations for Exploration
*   Server startup: `cmd/insid/main.go`
*   Service layer & HTTP handlers: `internal/service/service.go`
*   Raft FSM logic: `internal/rft/fsm.go`
*   Client CLI: `cmd/insic/main.go`
*   Go client library: `client/client.go`
*   Configuration loading: `internal/config/config.go`
*   Data models: `models/data.go`

## 7. Further Exploration
*   Dive deeper into the `github.com/hashicorp/raft` library and its concepts (log compaction, leadership transfer, membership changes).
*   Explore the `github.com/InsulaLabs/insula/tkv` library's implementation, particularly its BadgerDB usage and caching strategies.
*   Understand the cryptographic details of `github.com/InsulaLabs/insula/security/badge` and `github.com/InsulaLabs/insula/security/sentinel`.
*   Implement more complex data interaction scenarios using the Go client library.
*   Consider metrics, monitoring, and more advanced deployment strategies for a production environment.

---
This document provides an overview. For specific details, always refer to the source code and any further focused documentation. 